import torch.cuda
from torchinfo import summary

from project_utils import *


def other_simple():
    # torch.Tensorï¼šä½¿ç”¨å®ƒåˆ›å»ºçš„å¼ é‡å¯¹è±¡æ²¡æœ‰æŒ‡å®šæ•°æ®ç±»å‹ï¼Œå› æ­¤å…¶é»˜è®¤ä¸ºæµ®ç‚¹æ•°ç±»å‹ï¼ˆfloat32ï¼‰ï¼Œå…¶å€¼å–å†³äºå†…å­˜ä¸­çš„éšæœºæ•°æ®ã€‚
    # torch.tensorï¼šæ ¹æ®ç»™å®šçš„æ•°æ®åˆ›å»ºä¸€ä¸ªå¼ é‡å¯¹è±¡ï¼Œå¹¶è‡ªåŠ¨æ¨æ–­æ•°æ®ç±»å‹ã€‚å¯ä»¥æ¥å—å¤šç§æ•°æ®ç±»å‹ä½œä¸ºè¾“å…¥å‚æ•°ï¼Œä¾‹å¦‚åˆ—è¡¨ã€å…ƒç»„ã€æ•°ç»„ç­‰ã€‚
    # tensor([1.0561e-38, 1.0194e-38, 9.2755e-39, 8.9082e-39, 8.4490e-39, 9.2755e-39]) torch.Size([6])
    # ä¼ å…¥ä¸€ä¸ªæ•´æ•° n æ—¶ï¼Œtorch.Tensor è®¤è¯† n æ˜¯ä¸€ç»´å¼ é‡çš„å…ƒç´ ä¸ªæ•°ï¼Œå¹¶éšæœºåˆå§‹åŒ–
    print(torch.Tensor(6), torch.Tensor(6).shape)
    # tensor(6) torch.Size([])
    # torch.Size([]) å’Œ torch.Size([0]) çš„åŒºåˆ«ï¼Œå‰è€…æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œä¸èƒ½è¿­ä»£ï¼Œåè€…æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œåªä¸è¿‡æ²¡æœ‰å…ƒç´ ï¼Œå½¢å¦‚ []ï¼Œå¯ä»¥è¿­ä»£
    # torch.tensoråˆ™ä¼šå°†nè§†ä½œä¸€ä¸ªæ•°å­—è€Œä¸æ˜¯å…ƒç´ ä¸ªæ•°ã€‚ä¾‹å¦‚ï¼š
    print(torch.tensor(6), torch.tensor(6).shape)
    # tensor([1., 2., 3.])
    print(torch.Tensor([1, 2, 3]))
    # tensor([1, 2, 3])
    print(torch.tensor([1, 2, 3]))

    # torch.arange(start=0, end, step=1)
    # åŒ python çš„ rangeï¼Œå‰å¼€åé—­
    assert_tensor_equal(torch.arange(1, 10, 3), [1, 4, 7])
    assert_tensor_equal(torch.arange(1, 1, 3), [])

    # reshape ä¼šè§†æƒ…å†µè¿”å› view æˆ–è€…æ˜¯æ–°çš„ copyï¼Œä¸èƒ½ä¾èµ–è¿™ä¸€ç‚¹
    # ä½† view å¯èƒ½ä¼šå¤±è´¥ï¼Œå› ä¸ºå¿…é¡»æ˜¯ strided
    assert_tensor_shape_equal(torch.arange(1, 10).reshape((3, 3)), (3, 3))

    a = get_a_sample_tensor((2, 3))
    # transpose æ˜¯å°†ä¸¤ä¸ªç»´åº¦è¿›è¡Œè½¬ç½®ï¼Œå¦‚æœ strided åˆ™åŸä½ï¼Œå¦åˆ™è¿”å› copy
    b = a.transpose(0, 1)
    # view ç›¸å½“äºæ˜¯ reshapeï¼Œæ³¨æ„ view å¯èƒ½ä¼šæ‰§è¡Œå¤±è´¥ï¼Œå¦‚æœ tensor ä¸æ˜¯ stridedï¼Œè€Œ reshape åˆ™ä¸ä¼šå¤±è´¥ï¼Œå¦‚æœä¸æ˜¯ strided å°±è¿”å› copy
    c = a.view(3, 2)
    assert_tensor_equal(b, [[0, 3], [1, 4], [2, 5]])
    assert_tensor_equal(c, [[0, 1], [2, 3], [4, 5]])
    assert_tensor_shape_equal(b, c)
    assert not torch.equal(b, c)

    # linalg.vector_norm: Expected a floating point or complex tensor as input. Got Long
    # å¦‚æœä¸åŠ  dimï¼Œå°±ä¸èƒ½ keepdim=Trueï¼Œå› ä¸ºè¿™ä¸ªæ—¶å€™å·²ç»æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œæ²¡æ³•æ‰©å±•ç»´åº¦ï¼Œå·²ç»æ¢å¤ä¸äº†äº†
    a = get_a_sample_tensor((2, 3))
    # å¦‚æœ norm ä¸ä¼ å…¥ä»»ä½•å‚æ•°ï¼Œåˆ™ç­‰ä»·äºå°† tensor å±•å¹³ï¼Œè¿”å›æ‰€æœ‰å…ƒç´ çš„ norm
    # norm(p: Optional[Union[float, str]] = "fro", dim=None, keepdim=False, dtype=None,
    assert_tensor_close(a.norm(), 7.4162)
    assert_tensor_close(a.norm(keepdim=True), [[7.4162]])
    assert_tensor_close(a.norm(dim=0), [3.0000, 4.1231, 5.3852])
    assert_tensor_close(a.norm(dim=(0, 1)), 7.4162)

    # tf.expand: å°†å¼ é‡å¹¿æ’­åˆ°æ–°çš„å½¢çŠ¶ã€‚
    # æ³¨æ„ï¼š åªèƒ½å¯¹ç»´åº¦å€¼ä¸º 1 çš„ç»´åº¦è¿›è¡Œæ‰©å±•ï¼Œæ— éœ€æ‰©å±•çš„ç»´åº¦ï¼Œç»´åº¦å€¼ä¸å˜ï¼Œå¯¹åº”ä½ç½®å¯å†™ä¸ŠåŸå§‹ç»´åº¦å¤§å°æˆ–ç›´æ¥å†™ä½œ -1
    # ä¸”æ‰©å±•çš„Tensorä¸ä¼šåˆ†é…æ–°çš„å†…å­˜ï¼Œåªæ˜¯åŸæ¥çš„åŸºç¡€ä¸Šåˆ›å»ºæ–°çš„è§†å›¾å¹¶è¿”å›ï¼Œè¿”å›çš„å¼ é‡å†…å­˜æ˜¯ä¸è¿ç»­çš„ã€‚
    # ç±»ä¼¼äºnumpyä¸­çš„broadcast_toå‡½æ•°çš„ä½œç”¨ã€‚å¦‚æœå¸Œæœ›å¼ é‡å†…å­˜è¿ç»­ï¼Œå¯ä»¥è°ƒç”¨contiguouså‡½æ•°ã€‚
    # expandå‡½æ•°å¯èƒ½å¯¼è‡´åŸå§‹å¼ é‡çš„å‡ç»´ï¼Œå…¶ä½œç”¨åœ¨å¼ é‡å‰é¢çš„ç»´åº¦ä¸Š(åœ¨tensorçš„ä½ç»´å¢åŠ æ›´å¤šç»´åº¦)ï¼Œå› æ­¤é€šè¿‡expandå‡½æ•°å¯å°†å¼ é‡æ•°æ®å¤åˆ¶å¤šä»½ï¼ˆå¯ç†è§£ä¸ºæ²¿ç€ç¬¬ä¸€ä¸ªbatchçš„ç»´åº¦ä¸Šï¼‰
    # expand_as å¯è§†ä¸º expand çš„å¦ä¸€ç§è¡¨è¾¾ï¼Œå…¶sizeé€šè¿‡å‡½æ•°ä¼ é€’çš„ç›®æ ‡å¼ é‡çš„sizeæ¥å®šä¹‰ã€‚
    a = torch.arange(6).reshape((1, 1, 2, 3))
    assert_tensor_shape_equal(a.expand(2, -1, -1, -1), [2, 1, 2, 3])
    assert_tensor_shape_equal(a.squeeze(), [2, 3])
    assert_tensor_shape_equal(a.squeeze(0), [1, 2, 3])
    assert_tensor_shape_equal(a.squeeze((0, 1)), [2, 3])
    # å¦‚æœ squeeze æŸä¸€ä½ä¸æ˜¯ 1ï¼Œä¼šå…¼å®¹ä¸å¤„ç†ï¼Œå¯ä»¥ä¼ å…¥ tuple
    assert_tensor_shape_equal(a.squeeze((0, 1, 2)), [2, 3])
    # unsqueeze ä¸èƒ½ä¼ å…¥ tuple
    assert_tensor_shape_equal(a.unsqueeze(2), [1, 1, 1, 2, 3])

    # tensor.repeat()ï¼šå’Œexpand()ä½œç”¨ç±»ä¼¼ï¼Œå‡æ˜¯å°†tensorå¹¿æ’­åˆ°æ–°çš„å½¢çŠ¶ã€‚
    # æ³¨æ„ï¼šä¸å…è®¸ä½¿ç”¨ç»´åº¦ -1ï¼Œ1 å³ä¸ºä¸å˜ã€‚
    # å‰æ–‡æåŠexpandä»…èƒ½ä½œç”¨äºå•æ•°ç»´ï¼Œé‚£å¯¹äºéå•æ•°ç»´çš„æ‹“å±•ï¼Œé‚£å°±éœ€è¦å€ŸåŠ©äºrepeatå‡½æ•°äº†ã€‚
    # tensor.repeat(*sizes)
    # å‚æ•°*sizesæŒ‡å®šäº†åŸå§‹å¼ é‡åœ¨å„ç»´åº¦ä¸Šå¤åˆ¶çš„æ¬¡æ•°ã€‚æ•´ä¸ªåŸå§‹å¼ é‡ä½œä¸ºä¸€ä¸ªæ•´ä½“è¿›è¡Œå¤åˆ¶ï¼Œè¿™ä¸Numpyä¸­çš„repeatå‡½æ•°æˆªç„¶ä¸åŒï¼Œè€Œæ›´æ¥è¿‘äºtileå‡½æ•°çš„æ•ˆæœã€‚
    # ä¸expandä¸åŒï¼Œrepeatå‡½æ•°ä¼šçœŸæ­£çš„å¤åˆ¶æ•°æ®å¹¶å­˜æ”¾äºå†…å­˜ä¸­ã€‚repeatå¼€è¾Ÿäº†æ–°çš„å†…å­˜ç©ºé—´ï¼Œtorch.repeatè¿”å›çš„å¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„
    assert_tensor_shape_equal(a.repeat(1, 2, 3, 4), [1, 2, 6, 12])


# å„ç§ä¹˜æ³•
def check_mul():
    a = get_a_sample_tensor((2, 3))
    b = get_a_sample_tensor((2, 3))
    c = get_a_sample_tensor((3, 2))
    d = get_a_sample_tensor((1,))
    f = get_a_sample_tensor((3,))
    g = get_a_sample_tensor((3,))
    h = get_a_sample_tensor((2, 2, 3))
    x = get_a_sample_tensor((10, 1, 3, 4))
    y = get_a_sample_tensor((10, 3, 4, 5))
    z = get_a_sample_tensor((10, 3, 4, 5))

    # torch.mul é€ä¸ªå…ƒç´ ç›¸ä¹˜ï¼Œå¯ä»¥è¿›è¡Œå¹¿æ’­ï¼Œç®€å†™æ˜¯ *
    # å¹¿æ’­æœºåˆ¶ï¼š
    # 1.å¦‚æœç»´åº¦ä¸ªæ•°ä¸åŒï¼Œåˆ™åœ¨ç»´åº¦è¾ƒå°‘çš„å·¦è¾¹è¡¥1ï¼Œä½¿å¾—ç»´åº¦çš„ä¸ªæ•°ç›¸åŒã€‚
    # 2.å„ç»´åº¦çš„ç»´åº¦å¤§å°ä¸åŒæ—¶ï¼Œå¦‚æœæœ‰ç»´åº¦ä¸º1çš„ï¼Œç›´æ¥å°†è¯¥ç»´æ‹‰ä¼¸è‡³ç»´åº¦ç›¸åŒ
    assert_tensor_shape_equal(torch.mul(a, b), (2, 3))
    assert_tensor_shape_equal(torch.mul(a, d), (2, 3))
    # The size of tensor a (3) must match the size of tensor b (6) at non-singleton dimension 1
    # å¹¿æ’­åªèƒ½å¹¿æ’­æœ‰ 1 çš„ï¼Œä¸èƒ½æ™ºèƒ½çš„æ±‚å…¬å€æ•°
    # assert_tensor_shape_equal(torch.mul(a, e), (2, 3))

    # torch.matmul çŸ©é˜µç›¸ä¹˜ï¼Œç®€å†™æ˜¯ @
    # vector * vectorï¼Œå¾—åˆ°ä¸€ä¸ªæ ‡é‡
    # 1D * 1D = 0D
    assert_tensor_shape_equal(torch.matmul(f, g), tuple())
    # matrix * vectorï¼Œå¾—åˆ°ä¸€ä¸ª vector
    # 2D * 1D = 1D
    assert_tensor_shape_equal(torch.matmul(a, g), (2,))
    # 3D * vector = 2D
    assert_tensor_shape_equal(torch.matmul(h, g), (2, 2))
    # (..., a, b)D * (..., b, c)D = (..., a, c)Dï¼Œæœ€å2D å¿…é¡»æ»¡è¶³çŸ©é˜µä¹˜æ³•çš„æ¡ä»¶ï¼Œå‰é¢å¿…é¡»ä¸€è‡´ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¸­å¯ä»¥è§¦å‘å¹¿æ’­æœºåˆ¶
    assert_tensor_shape_equal(torch.matmul(x, y), (10, 3, 3, 5))
    assert_tensor_shape_equal(torch.matmul(x, z), (10, 3, 3, 5))

    # torch.mmï¼ŒçŸ©é˜µç›¸ä¹˜ï¼Œä¸ä¼šè¿›è¡Œå¹¿æ’­ï¼Œå¿…é¡»æ»¡è¶³çŸ©é˜µç›¸ä¹˜ç»´æ•°æ¡ä»¶,ä¸¤çŸ©é˜µæœ€å¤šæ˜¯2ç»´
    assert_tensor_shape_equal(torch.mm(a, c), (2, 2))

    # torch.bmmï¼Œæ‰¹çŸ©é˜µç›¸ä¹˜ï¼Œä¸ä¼šè¿›è¡Œå¹¿æ’­ï¼Œå¿…é¡»æ»¡è¶³çŸ©é˜µç›¸ä¹˜ç»´æ•°æ¡ä»¶ï¼Œa,bæœ€å¤šåªèƒ½3ç»´ï¼Œä¸”a,bä¸­å¿…é¡»åŒ…å«ç›¸åŒçš„çŸ©é˜µä¸ªæ•°å³a,bç¬¬ä¸€ç»´åº¦å¿…é¡»ç›¸åŒ
    assert_tensor_shape_equal(torch.bmm(torch.unsqueeze(a, 0), torch.unsqueeze(c, 0)), (1, 2, 2))

    # torch.dot(a,b)ï¼Œå‘é‡ç‚¹ç§¯ï¼Œä¸¤å‘é‡ç›¸ä¹˜ç›¸åŠ å¾—åˆ°ä¸€ä¸ªæ ‡é‡ï¼Œå¿…é¡»éƒ½æ˜¯ä¸€ç»´çš„
    assert_tensor_shape_equal(torch.dot(f, g), tuple())


# dim (int or tuple of python:ints) â€“ the dimension or dimensions to reduce.
# åœ¨å“ªäº›ç»´åº¦ä¸Šé¢åšå¹³å‡
# keepdim (bool) â€“ whether the output tensor has dim retained or not.
# æ˜¯å¦ä¿æŒå’ŒåŸæ¥ä¸€æ ·çš„ç»´åº¦ï¼Œé»˜è®¤æ˜¯ Falseï¼Œæ³¨æ„ç»´åº¦è¡¨ç¤ºåæ ‡ç³»ï¼Œå³ä¸‰ç»´è¡¨ç¤ºæœ‰ä¸‰ä¸ªæ–¹å‘ï¼Œshape è¡¨ç¤ºæ¯ä¸ªæ–¹å‘å¤šå¤§
def check_mean_op():
    # 2 * 3 * 4
    array = np.resize(np.array(range(1, 25)), (2, 3, 4))
    tensor = torch.tensor(array, dtype=torch.float)

    # æ³¨æ„ tensor.mean() æ˜¯æ ‡é‡ï¼Œä¸èƒ½ç”¨ to_tuple(0) æ¥æ¯”è¾ƒ
    assert_tensor_shape_equal(tensor.mean(), tuple())
    # å¦‚æœä¸åŠ  dimï¼Œå°±ä¸èƒ½ keepdim=Trueï¼Œå› ä¸ºè¿™ä¸ªæ—¶å€™å·²ç»æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œæ²¡æ³•æ‰©å±•ç»´åº¦ï¼Œå·²ç»æ¢å¤ä¸äº†äº†
    # tensor.mean(keepdim=True)
    assert_tensor_shape_equal(tensor.mean(dim=(1,)), (2, 4))
    assert_tensor_shape_equal(tensor.mean(dim=(1,), keepdim=True), (2, 1, 4))
    assert_tensor_shape_equal(tensor.mean(dim=(0,)), (3, 4))
    assert_tensor_shape_equal(tensor.mean(dim=(0,), keepdim=True), (1, 3, 4))
    assert_tensor_shape_equal(tensor.mean(dim=(0, 1)), 4)
    assert_tensor_shape_equal(tensor.mean(dim=(0, 1), keepdim=True), (1, 1, 4))


# unbiased (bool) â€“ whether to use Besselâ€™s correction
# æ˜¯å¦å¼€å¯ è´å¡å°”æ ¡æ­£ï¼Œé»˜è®¤æ˜¯ Trueï¼Œå½¢å¦‚ np.std() ä¸­çš„ ddof=1ï¼ŒMeans Delta Degrees of Freedom
# åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œè´å¡å°”æ ¡æ­£æ˜¯åœ¨æ ·æœ¬çš„æ–¹å·®å’Œæ ‡å‡†å·®çš„å…¬å¼ä¸­ç”¨n-1æ¥ä»£æ›¿nã€‚è¿™ä¸ªæ–¹æ³•æ ¡æ­£äº†æ ·æœ¬æ–¹å·®/æ ·æœ¬æ ‡å‡†å·®ï¼Œä¸æ€»ä½“æ–¹å·®/æ ·æœ¬æ ‡å‡†å·®ä¹‹é—´çš„è¯¯å·®ã€‚
# ä¸¾ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœä¸€ä¸ªæ•°æ®é›†æ»¡è¶³é«˜æ–¯åˆ†å¸ƒï¼ˆNormal Distributionï¼‰ï¼Œé‚£å½“æˆ‘ä»¬æå–æ ·æœ¬çš„æ—¶å€™ï¼Œæ•°æ®åŸºæœ¬ä¸Šä¼šé›†ä¸­åœ¨ä¸­é—´çš„éƒ¨åˆ†ï¼Œè€Œè¾¹ç¼˜å€¼çš„æ•°ç›®å¯èƒ½ä¼šæ¯”è¾ƒå°‘ï¼Œ
# æ‰€ä»¥æœ€åå¾—åˆ°çš„æ ·æœ¬æ–¹å·®å’Œæ ·æœ¬æ ‡å‡†å·®ä¼šæ¯”æ€»ä½“è¦å°ã€‚ä¸ºäº†ä¿®æ­£è¿™ä¸ªåå·®ï¼Œåœ¨è®¡ç®—æ ·æœ¬çš„æ–¹å·®å’Œæ ‡å‡†å·®æ—¶ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ n-1 ä»£æ›¿ nã€‚è¿™æ ·å¤„ç†åæœ€ç›´æ¥çš„ç»“æœæ˜¯ï¼Œå…¬å¼ä¸­çš„åˆ†æ¯å˜å°ï¼Œå¾—åˆ°çš„ç»“æœå°†ä¼šå˜å¤§ï¼Œèƒ½å¤Ÿæ›´åŠ å‡†ç¡®åœ°é€šè¿‡è¯¥æ ·æœ¬é¢„æµ‹æ€»ä½“çš„æƒ…å†µã€‚
def check_std_op():
    # 2 * 3 * 4
    array = np.array(range(1, 4))
    # sqrt((1 + 0 + 1) / 3) = 0.816496580927726
    assert_close(np.std(array), 0.8165)
    # sqrt((1 + 0 + 1) / (3 - 1)) = 1.0
    assert_equal(np.std(array, ddof=1), 1)

    tensor = torch.tensor(array, dtype=torch.float)
    assert_tensor_equal(tensor.std(), 1)
    assert_tensor_close(tensor.std(unbiased=False), 0.8165)


# per channel
# æ¯ä¸€ä¸ª C =  channel å†…ï¼Œå¯¹æ•´ä¸ª mini-batch çš„å½’ä¸€åŒ–ï¼Œæ”¶ç¼©å‡ ç»´ï¼Œå°±æœ‰å¤šå°‘å¯¹ (Î³, Î²)
# é™¤äº† C éƒ½æ”¶ç¼©
# normalized_shape = C, æ¯ä¸€ä¸ª normalized_shape æœ‰ä¸€å¯¹ (Î³, Î²)
# (N, C) -> (1, C)
# nlp, C æ˜¯ embedding çš„ç»´åº¦ï¼Œl æ˜¯æ ·æœ¬é•¿åº¦
# (N, C, L) -> (1, C, 1)
# cvï¼ŒC æ˜¯é€šé“æ•°ï¼ŒHã€W æ˜¯é•¿å®½
# (N, C, H, W) -> (1, C, 1, 1)
def check_batch_norm():
    # Applies Batch Normalization over a 2D or 3D input
    # ä¸€ç»´ BNï¼Œé’ˆå¯¹ 2d æˆ–è€… 3d input
    # num_features â€“ number of features or channels C of the input
    # eps â€“ a value added to the denominator for numerical stability. Default: 1e-5
    # momentum â€“ the value used for the running_mean and running_var computation.
    # Can be set to None for cumulative moving average (i.e. simple average). Default: 0.1
    # affine â€“ a boolean value that when set to True, this module has learnable affine parameters. Default: True
    # track_running_stats â€“ a boolean value that when set to True, this module tracks the running mean and variance,
    # and when set to False, this module does not track such statistics, and initializes statistics buffers
    # running_mean and running_var as None. When these buffers are None, this module always uses batch statistics.
    # in both training and eval modes. Default: True
    c_index = 1

    # (N, C) -> (1, C)
    shape = (2, 3)
    c = shape[c_index]
    other_index_list = get_index_exclude_index(shape, c_index)
    x = get_a_sample_tensor(shape)
    x_bn = nn.BatchNorm1d(c)(x)
    x_bn_check = get_tensor_norm(x, other_index_list)
    assert_tensor_close(x_bn, x_bn_check)

    # (N, C, L) -> (1, C, 1)
    shape = (2, 3, 4)
    c = shape[c_index]
    x = get_a_sample_tensor(shape)
    x_bn = nn.BatchNorm1d(c)(x)
    other_index_list = get_index_exclude_index(shape, c_index)
    # å¯¹æ¯” nn.BatchNorm1d(c)ï¼Œå¯¹é™¤äº† c ä»¥å¤–çš„ç»´åº¦å…¨éƒ¨æ”¶ç¼©
    x_bn_check = get_tensor_norm(x, other_index_list)
    assert_tensor_close(x_bn, x_bn_check)

    # Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs with additional channel dimension)
    # äºŒç»´ BNï¼Œé’ˆå¯¹ 4d input
    # (N, C, H, W) -> (1, C, 1, 1)
    shape = (2, 3, 4, 5)
    c = shape[c_index]
    other_index_list = get_index_exclude_index(shape, c_index)
    x = get_a_sample_tensor(shape)
    x_bn = nn.BatchNorm2d(c)(x)
    # å¯¹æ¯” nn.BatchNorm1d(c)ï¼Œå¯¹é™¤äº† c ä»¥å¤–çš„ç»´åº¦å…¨éƒ¨æ”¶ç¼©
    x_bn_check = get_tensor_norm(x, other_index_list)
    assert_tensor_close(x_bn, x_bn_check)


# per sample per layer
# åªå¯¹æŒ‡å®šçš„åå‡ ç»´åšæ”¶ç¼©
# normalized_shape = æœ€åå‡ ä¸ªç»´åº¦ï¼Œæ”¶ç¼©å‡ ç»´ï¼Œå°±æœ‰å¤šå°‘å¯¹ (Î³, Î²)
# (N, C) -> (N, 1)
# nlp
# (N, L, C) -> (N, L, 1)
# cv
# (N, C, H, W) -> (N, 1, 1, 1)

# NLP Example
# embedding = torch.randn(batch, sentence_length, embedding_dim)
# layer_norm = nn.LayerNorm(embedding_dim)
# layer_norm(embedding)

# Image Example
# N, C, H, W = 20, 5, 10, 10
# input = torch.randn(N, C, H, W)
# layer_norm = nn.LayerNorm([C, H, W])
def check_layer_norm():
    # Applies Layer Normalization over a mini-batch of inputs
    # ä»»æ„ç»´ LN
    # å’Œ BN ä¸åŒï¼ŒLN ä¸­çš„ mean å’Œ std æ˜¯ç”±å½“å‰æ ·æœ¬å†³å®šçš„ï¼Œä¸æ›´æ–°

    # (N, C) -> (N, 1)
    shape = (2, 3)
    last_dimensions = [-1]
    normalized_shape = to_tuple(get_list_from_index(shape, last_dimensions))
    x = get_a_sample_tensor(shape)
    x_bn = nn.LayerNorm(normalized_shape)(x)
    x_bn_check = get_tensor_norm(x, last_dimensions)
    assert_tensor_close(x_bn, x_bn_check)

    # (N, L, C) -> (N, L, 1)
    shape = (2, 3, 4)
    last_dimensions = [-1]
    normalized_shape = to_tuple(get_list_from_index(shape, last_dimensions))
    x = get_a_sample_tensor(shape)
    x_bn = nn.LayerNorm(normalized_shape)(x)
    x_bn_check = get_tensor_norm(x, last_dimensions)
    assert_tensor_close(x_bn, x_bn_check)

    # (N, L, C) -> (N, 1, 1)
    shape = (2, 3, 4)
    last_dimensions = [-2, -1]
    normalized_shape = to_tuple(get_list_from_index(shape, last_dimensions))
    x = get_a_sample_tensor(shape)
    x_bn = nn.LayerNorm(normalized_shape)(x)
    x_bn_check = get_tensor_norm(x, last_dimensions)
    assert_tensor_close(x_bn, x_bn_check)

    # (N, C, H, W) -> (N, 1, 1, 1)
    shape = (2, 3, 4, 5)
    last_dimensions = [-3, -2, -1]
    normalized_shape = to_tuple(get_list_from_index(shape, last_dimensions))
    x = get_a_sample_tensor(shape)
    x_bn = nn.LayerNorm(normalized_shape)(x)
    x_bn_check = get_tensor_norm(x, last_dimensions)
    assert_tensor_close(x_bn, x_bn_check, abs_tol=1e-6)


# per sample per channel
# ä¸€å®šä¿ç•™ Cï¼Œå¦‚æœæœ‰ Nï¼Œä¹Ÿä¿ç•™ Nï¼Œæ”¶ç¼©å‡ ç»´ï¼Œå°±æœ‰å¤šå°‘å¯¹ (Î³, Î²)
# (C, L) -> (C, 1)
# nlp
# (N, L, C) -> (N, L, 1)
# (C, H, W) -> (C, 1, 1)
# cv
# (N, C, H, W) -> (N, C, 1, 1)
def check_instance_norm():
    # (C, L) -> (C, 1)
    c_index = 0
    shape = (2, 3)
    c = shape[c_index]
    x = get_a_sample_tensor(shape)
    x_bn = nn.InstanceNorm1d(c)(x)
    x_bn_check = get_tensor_norm(x, 1)
    assert_tensor_close(x_bn, x_bn_check)

    # (N, L, C) -> (N, L, 1)
    c_index = 1
    shape = (2, 3, 4)
    c = shape[c_index]
    x = get_a_sample_tensor(shape)
    x_bn = nn.InstanceNorm1d(c)(x)
    x_bn_check = get_tensor_norm(x, 2)
    assert_tensor_close(x_bn, x_bn_check)

    # (C, H, W) -> (C, 1, 1)
    c_index = 0
    shape = (2, 3, 4)
    c = shape[c_index]
    x = get_a_sample_tensor(shape)
    x_bn = nn.InstanceNorm2d(c)(x)
    x_bn_check = get_tensor_norm(x, (1, 2))
    assert_tensor_close(x_bn, x_bn_check, abs_tol=1e-6)

    # (N, C, H, W) -> (N, C, 1, 1)
    c_index = 1
    shape = (2, 3, 4, 5)
    c = shape[c_index]
    x = get_a_sample_tensor(shape)
    x_bn = nn.InstanceNorm2d(c)(x)
    x_bn_check = get_tensor_norm(x, (2, 3))
    assert_tensor_close(x_bn, x_bn_check, abs_tol=1e-6)


# per sample per group
# group_norm æ˜¯ layer_norm çš„ç‰¹æ®Šæƒ…å†µ
# åªå¯¹æŒ‡å®šçš„åå‡ ç»´åšæ”¶ç¼©
# normalized_shape = æœ€åå‡ ä¸ªç»´åº¦ï¼Œæ”¶ç¼©å‡ ç»´ï¼Œå°±æœ‰å¤šå°‘å¯¹ (Î³, Î²)
# num_channels must be divisible by num_groups
# num_groups å¿…é¡»èƒ½å‡åˆ† C
# (N, C) -> num_groups * (N, 1)
# (N, C, L) -> num_groups * (N, 1, 1)
# (N, C, H, W) -> num_groups * (N, 1, 1, 1)

# NLP Example
# embedding = torch.randn(batch, sentence_length, embedding_dim)
# layer_norm = nn.LayerNorm(embedding_dim)
# layer_norm(embedding)

# Image Example
# N, C, H, W = 20, 5, 10, 10
# input = torch.randn(N, C, H, W)
# layer_norm = nn.LayerNorm([C, H, W])
def check_group_norm():
    c_index = 1

    for num_groups in (1, 2, 4):
        # (N, C) -> num_groups * (N, 1)
        shape = (2, 4)
        c = shape[c_index]
        x = get_a_sample_tensor(shape)
        x_bn = nn.GroupNorm(num_groups, c)(x)
        x_bn_check_list = list()
        for sub_x in torch.split(x, c // num_groups, dim=c_index):
            sub_x_bn_check = get_tensor_norm(sub_x, 1)
            x_bn_check_list.append(sub_x_bn_check)
        # cat æ˜¯ split çš„é€†æ“ä½œï¼Œè¿™é‡Œç”¨äº split ä¸€æ ·çš„ dim
        x_bn_check = torch.cat(x_bn_check_list, dim=c_index)
        assert_tensor_close(x_bn, x_bn_check)

        # (N, C, L) -> num_groups * (N, 1, 1)
        shape = (2, 4, 5)
        c = shape[c_index]
        x = get_a_sample_tensor(shape)
        x_bn = nn.GroupNorm(num_groups, c)(x)
        x_bn_check_list = list()
        for sub_x in torch.split(x, c // num_groups, dim=c_index):
            sub_x_bn_check = get_tensor_norm(sub_x, (1, 2))
            x_bn_check_list.append(sub_x_bn_check)
        # cat æ˜¯ split çš„é€†æ“ä½œï¼Œè¿™é‡Œç”¨äº split ä¸€æ ·çš„ dim
        x_bn_check = torch.cat(x_bn_check_list, dim=c_index)
        assert_tensor_close(x_bn, x_bn_check)

        # (N, C, H, W) -> num_groups * (N, 1, 1, 1)
        shape = (2, 4, 5, 6)
        c = shape[c_index]
        x = get_a_sample_tensor(shape)
        x_bn = nn.GroupNorm(num_groups, c)(x)
        x_bn_check_list = list()
        for sub_x in torch.split(x, c // num_groups, dim=c_index):
            sub_x_bn_check = get_tensor_norm(sub_x, (1, 2, 3))
            x_bn_check_list.append(sub_x_bn_check)
        # cat æ˜¯ split çš„é€†æ“ä½œï¼Œè¿™é‡Œç”¨äº split ä¸€æ ·çš„ dim
        x_bn_check = torch.cat(x_bn_check_list, dim=c_index)
        assert_tensor_close(x_bn, x_bn_check, abs_tol=1E-6)


# https://pytorch.org/docs/2.2/generated/torch.nn.utils.parametrizations.weight_norm.html#torch.nn.utils.parametrizations.weight_norm
# w = g * v / |v|
# å°† weight æƒé‡çŸ©é˜µæ‹†æˆä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯å¹…å€¼ gï¼Œå¦å¤–ä¸€ä¸ªæ˜¯æ–¹å‘ v / |v|
# v = w, g = |w| æ‰€ä»¥ g * v / |v| = w
# å…¶ä¸­ï¼Œ|w| = w.norm(dim=1, keepdim=True))
def check_weight_norm():
    shape = (3, 5)
    x = get_a_sample_tensor(shape)

    linear = nn.Linear(5, 7, bias=False)
    y = linear(x)
    assert_tensor_shape_equal(y, (3, 7))

    weight = linear.weight
    assert_tensor_shape_equal(weight, (7, 5))
    assert_tensor_shape_equal(weight.T, (5, 7))
    y_check = torch.matmul(x, weight.T)
    assert_tensor_equal(y, y_check)

    wn_linear = torch.nn.utils.parametrizations.weight_norm(linear)
    # weight æœ¬èº«æ˜¯ä¸å˜çš„ï¼Œåªæ˜¯æ¯æ¬¡è¿›è¡Œ forward ä¹‹å‰ä¼šåˆ©ç”¨ä¸‹é¢çš„ g å’Œ v æ¥è®¡ç®—
    assert_tensor_equal(weight, wn_linear.weight)
    wn_y = wn_linear(x)
    assert_tensor_equal(y, wn_y)

    # æ¯ä¸ª x æ˜¯ä¸€ä¸ª 5D vectorï¼Œå› æ­¤æ¯ä¸ª w æ˜¯ä¸€ä¸ª 5D vectorï¼Œå› ä¸ºè¾“å‡ºå±‚æ˜¯ 7ï¼Œ å› æ­¤éœ€è¦æœ‰ 7 ä¸ª w ç»„æˆä¸€ä¸ª W
    # è¿™é‡ŒçŸ©é˜µèŒƒæ•° |W| æ˜¯è¡¨ç¤ºå°†æ¯ä¸ª w è¿›è¡Œå½’ä¸€åŒ– (L2 èŒƒæ•°)ï¼Œè¿™æ · W @ W = çš„å¯¹è§’çº¿éƒ½æ˜¯ 1
    weight_norm = weight.norm(dim=1, keepdim=True)
    # å¦å¤–ä¸€ç§è®¡ç®—çŸ©é˜µèŒƒæ•°çš„æ–¹æ³•
    assert_tensor_equal(weight_norm,
                        torch.tensor([get_vector_norm(weight[i, :]) for i in torch.arange(weight.shape[0])])
                        .unsqueeze(-1))
    assert_tensor_shape_equal(weight_norm, (7, 1))
    weight_direction = weight / weight_norm

    assert_tensor_close(weight_direction.norm(dim=-1), [1.0] * 7)
    assert_tensor_close((weight_direction @ weight_direction.T).diagonal().sum(), 7.0)

    # # åˆ©ç”¨çŸ©é˜µèŒƒæ•°å½’ä¸€åŒ–ï¼Œä½¿å¾— weight_v è¡¨ç¤ºæ–¹å‘ï¼ŒçŸ©é˜µèŒƒæ•° = çŸ©é˜µå„é¡¹å…ƒç´ å¹³æ–¹å’Œå†å¼€æ ¹å·ï¼Œæ³¨æ„è¿™é‡Œç”¨çš„å¹¶ä¸æ˜¯è¿™ä¸ªèŒƒæ•°
    weight_v = wn_linear.parametrizations.weight.original1
    weight_g = wn_linear.parametrizations.weight.original0
    assert_tensor_shape_equal(weight_g, (7, 1))
    assert_tensor_shape_equal(weight_v, (7, 5))

    assert_tensor_equal(weight_norm, weight_g)
    assert_tensor_equal(weight, weight_v)


def check_gpu(_with_speed=False):
    # 2.2.1+cu121
    # print(torch.__version__)

    assert torch.cuda.is_available()
    assert torch.backends.cudnn.enabled

    if _with_speed:
        dimension = 5000

        # i9-9900K
        # spent 111.40064930915833
        # i9-14900KF
        # spent 40.08144783973694
        # device = torch.device("cpu")

        # 2080Ti
        # spent 4.195726633071899
        # 4090
        # spent 2.9713356494903564
        device = torch.device("cuda")

        x = torch.rand((dimension, dimension), dtype=torch.float32)
        y = torch.rand((dimension, dimension), dtype=torch.float32)

        x = x.to(device)
        y = y.to(device)

        start_time = time.time()
        for i in range(10000):
            # noinspection PyUnusedLocal
            z = x * y
        end_time = time.time()

        # æ€»æ˜¾å­˜ (GB):      2.0
        # torch æ˜¾å­˜ (GB):  0.4
        # tensor æ˜¾å­˜ (GB): 0.3
        print_gpu_memory_summary()

        print("spent {}".format(end_time - start_time))


# æ£€æŸ¥ half çš„ç”¨æ³•ï¼Œå…¶å®å°±æ˜¯è½¬åŒ–ä¸º float 16
def check_half():
    float_64 = torch.tensor([3.1415926], dtype=torch.float64)
    float_32 = torch.tensor([3.1415926], dtype=torch.float32)
    float_16 = torch.tensor([3.1415926], dtype=torch.float16)

    # tensor([3.1416], dtype=torch.float64)
    # tensor([3.1406], dtype=torch.float16)
    # tensor([3.1406], dtype=torch.float16)
    print(float_64)
    print(float_64.half())
    print(float_64.half().half())

    # tensor([3.1416])
    # tensor([3.1406], dtype=torch.float16)
    # tensor([3.1406], dtype=torch.float16)
    print(float_32)
    print(float_32.half())
    print(float_32.half().half())

    # tensor([3.1406], dtype=torch.float16)
    # tensor([3.1406], dtype=torch.float16)
    # tensor([3.1406], dtype=torch.float16)
    print(float_16)
    print(float_16.half())
    print(float_16.half().half())


@func_timer(arg=True)
def check_chatglm3():
    from transformers import AutoModel, AutoTokenizer
    # trust_remote_code è¡¨ç¤ºç›¸ä¿¡æœ¬åœ°çš„ä»£ç ï¼Œè€Œä¸æ˜¯è¡¨ç¤ºåŒæ„ä¸‹è½½è¿œç¨‹ä»£ç ï¼Œä¸è¦æ··æ·†
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=CHATGLM3_6B_model_dir,
                                              trust_remote_code=True)
    # <class 'transformers_modules.chatglm3-6b.tokenization_chatglm.ChatGLMTokenizer'>
    print(type(tokenizer))
    # ['SPECIAL_TOKENS_ATTRIBUTES', 'add_special_tokens', 'add_tokens', 'added_tokens_decoder', 'added_tokens_encoder',
    # 'additional_special_tokens', 'additional_special_tokens_ids', 'all_special_ids', 'all_special_tokens',
    # 'all_special_tokens_extended', 'apply_chat_template', 'as_target_tokenizer', 'batch_decode', 'batch_encode_plus',
    # 'bos_token', 'bos_token_id', 'build_chat_input', 'build_inputs_with_special_tokens', 'build_single_message',
    # 'chat_template', 'clean_up_tokenization', 'clean_up_tokenization_spaces', 'cls_token', 'cls_token_id',
    # 'convert_added_tokens', 'convert_ids_to_tokens', 'convert_tokens_to_ids', 'convert_tokens_to_string',
    # 'create_token_type_ids_from_sequences', 'decode', 'default_chat_template', 'deprecation_warnings',
    # 'encode', 'encode_plus', 'eos_token', 'eos_token_id', 'from_pretrained', 'get_added_vocab', 'get_command',
    # 'get_prefix_tokens', 'get_special_tokens_mask', 'get_vocab', 'init_inputs', 'init_kwargs', 'is_fast',
    # 'mask_token', 'mask_token_id', 'max_len_sentences_pair', 'max_len_single_sentence', 'max_model_input_sizes',
    # 'model_input_names', 'model_max_length', 'name', 'name_or_path', 'num_special_tokens_to_add', 'pad',
    # 'pad_token', 'pad_token_id', 'pad_token_type_id', 'padding_side', 'prepare_for_model', 'prepare_for_tokenization',
    # 'prepare_seq2seq_batch', 'pretrained_init_configuration', 'pretrained_vocab_files_map', 'push_to_hub',
    # 'register_for_auto_class', 'sanitize_special_tokens', 'save_pretrained', 'save_vocabulary', 'sep_token',
    # 'sep_token_id', 'slow_tokenizer_class', 'special_tokens', 'special_tokens_map', 'special_tokens_map_extended',
    # 'split_special_tokens', 'tokenize', 'tokenizer', 'tokens_trie', 'truncate_sequences', 'truncation_side',
    # 'unk_token', 'unk_token_id', 'verbose', 'vocab_file', 'vocab_files_names', 'vocab_size']
    print_dir(tokenizer)

    dictionary = tokenizer.get_vocab()
    # <class 'dict'> 64796 True
    # å­—å…¸
    print(type(dictionary), len(dictionary), "æœˆå…‰" in dictionary)

    # encode å°±æ˜¯ encode_plus çš„ä¸€éƒ¨åˆ†
    # return self.encode_plus()["input_ids"]
    # [64790, 64792, 34211, 51225, 34886, 30930]
    # print(tokenizer.encode('æˆ‘çˆ±æˆ‘è€å©†.'))

    # {'input_ids': [0, 0, 64790, 64792, 34211, 51225, 34886, 30930, 34211, 34886, 54532, 55266, 54678, 30930, 2],
    # 'token_type_ids': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
    # 'attention_mask': [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    # 'position_ids': [0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}
    # æ”¯æŒä¼ ä¸€å¥è¯æˆ–è€…ä¸¤å¥è¯ï¼Œå¦‚æ¯å¥è¯çš„å¼€å¤´æœ‰ "_"
    # å¦‚æœè¦æƒ³æ‰¹é‡ç¼–ç ï¼Œè°ƒç”¨ batch_encode_plusï¼Œä¼šå¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œè¡¨ç¤º batch
    sen_code = tokenizer.encode_plus('æˆ‘çˆ±æˆ‘è€å©†.', 'æˆ‘è€å©†æ˜¯é™ˆå¹³.', truncation=True, max_length=15,
                                     padding="max_length", return_token_type_ids=True, return_special_tokens_mask=True)
    print(sen_code)
    # ['', '', '[gMASK]', 'sop', 'â–æˆ‘', 'çˆ±æˆ‘', 'è€å©†', '.', 'â–æˆ‘', 'è€å©†', 'æ˜¯', 'é™ˆ', 'å¹³', '.', '']
    print(tokenizer.convert_ids_to_tokens(sen_code['input_ids']))

    sen_code = tokenizer.encode_plus('ä½ è¯´ä»€ä¹ˆ.', 'è¿™ä¸ªè¯¾ç¨‹å¤ªéš¾å­¦äº†.')
    print(sen_code)
    # ['[gMASK]', 'sop', 'â–ä½ ', 'è¯´ä»€ä¹ˆ', '.', 'â–è¿™ä¸ª', 'è¯¾ç¨‹', 'å¤ªéš¾', 'å­¦äº†', '.', '']
    print(tokenizer.convert_ids_to_tokens(sen_code['input_ids']))

    # é€šè¿‡æŸ¥çœ‹ config.jsonï¼Œtorch_dtype = float16"ï¼Œå› æ­¤è¿™é‡Œç”¨ä¸ç”¨ half éƒ½å¯ä»¥
    model = AutoModel.from_pretrained(CHATGLM3_6B_model_dir, trust_remote_code=True).cuda()
    # <class 'transformers_modules.chatglm3-6b.modeling_chatglm.ChatGLMForConditionalGeneration'>
    print(type(model))
    # ['T_destination', 'active_adapter', 'active_adapters', 'add_adapter', 'add_memory_hooks', 'add_model_tags',
    # 'add_module', 'apply', 'assisted_decoding', 'base_model', 'base_model_prefix', 'beam_sample', 'beam_search',
    # 'bfloat16', 'buffers', 'call_super_init', 'can_generate', 'chat', 'children', 'compile',
    # 'compute_transition_scores', 'config', 'config_class', 'constrained_beam_search', 'contrastive_search', 'cpu',
    # 'create_extended_attention_mask_for_decoder', 'cuda', 'device', 'disable_adapters', 'disable_input_require_grads',
    # 'double', 'dtype', 'dummy_inputs', 'dump_patches', 'enable_adapters', 'enable_input_require_grads',
    # , 'eval', 'extra_repr', 'float', 'floating_point_ops', 'forward', 'framework', 'from_pretrained', 'generate',
    # 'generation_config', 'get_adapter_state_dict', 'get_buffer', 'get_extended_attention_mask', 'get_extra_state',
    # 'get_head_mask', 'get_input_embeddings', 'get_masks', 'get_memory_footprint', 'get_output_embeddings',
    # 'get_parameter', 'get_position_embeddings', 'get_position_ids', 'get_submodule', 'gradient_checkpointing_disable',
    # 'gradient_checkpointing_enable', 'greedy_search', 'group_beam_search', 'half', 'init_weights',
    # 'invert_attention_mask', 'ipu', 'is_gradient_checkpointing', 'is_parallelizable', 'load_adapter',
    # 'load_state_dict', 'main_input_name', 'max_sequence_length', 'model_tags', 'modules', 'name_or_path',
    # 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_parameters', 'parameters',
    # 'post_init', 'prepare_inputs_for_generation', 'process_response', 'prune_heads', 'push_to_hub', 'quantize',
    # 'quantized', 'register_backward_hook', 'register_buffer', 'register_for_auto_class', 'register_forward_hook',
    # 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook',
    # 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook',
    # 'requires_grad_', 'reset_memory_hooks_state', 'resize_position_embeddings', 'resize_token_embeddings',
    # 'retrieve_modules_from_names', 'reverse_bettertransformer', 'sample', 'save_pretrained', 'set_adapter',
    # 'set_extra_state', 'set_input_embeddings', 'share_memory', 'state_dict', 'stream_chat', 'stream_generate',
    # 'supports_gradient_checkpointing', 'tie_weights', 'to', 'to_bettertransformer', 'to_empty', 'train', 'training',
    # 'transformer', 'type', 'warn_if_padding_and_no_attention_mask', 'warnings_issued', 'xpu', 'zero_grad']
    print_dir(model)

    # model = AutoModel.from_pretrained(CHATGLM3_6B_model_dir, trust_remote_code=True).half().cuda()
    total_parameters = model.num_parameters()
    # æ€»æ˜¾å­˜ (GB):      13.22
    # torch æ˜¾å­˜ (GB):  11.66
    # tensor æ˜¾å­˜ (GB): 11.66
    print_gpu_memory_summary()

    # å‚æ•°é‡ï¼š6243584000ï¼Œå ç”¨æ˜¾å­˜: 11.63 GB
    print(F"å‚æ•°é‡ï¼š{total_parameters}ï¼Œå ç”¨æ˜¾å­˜: {round(total_parameters * 2 / 1024 ** 3, 2)} GB")

    # ================================================================================
    # Layer (type:depth-idx)                                  Param #
    # ================================================================================
    # ChatGLMForConditionalGeneration                         --
    # â”œâ”€ChatGLMModel: 1-1                                     --
    # â”‚    â””â”€Embedding: 2-1                                   --
    # â”‚    â”‚    â””â”€Embedding: 3-1                              266,338,304
    # â”‚    â””â”€RotaryEmbedding: 2-2                             --
    # â”‚    â””â”€GLMTransformer: 2-3                              --
    # â”‚    â”‚    â””â”€ModuleList: 3-2                             5,710,903,296
    # â”‚    â”‚    â””â”€RMSNorm: 3-3                                4,096
    # â”‚    â””â”€Linear: 2-4                                      266,338,304
    # ================================================================================
    # Total params: 6,243,584,000
    # Trainable params: 6,243,584,000
    # Non-trainable params: 0
    # ================================================================================
    # æ³¨æ„ï¼Œéœ€è¦ç»™ input æ‰èƒ½çŸ¥é“æ•´ä¸ªçš„å‚æ•°é‡
    summary(model)

    # ChatGLMForConditionalGeneration(
    #   (transformer): ChatGLMModel(
    #     (embedding): Embedding(
    #       (word_embeddings): Embedding(65024, 4096)
    #     )
    #     (rotary_pos_emb): RotaryEmbedding()
    #     (encoder): GLMTransformer(
    #       (layers): ModuleList(
    #         (0-27): 28 x GLMBlock(
    #           (input_layernorm): RMSNorm()
    #           (self_attention): SelfAttention(
    #             (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)
    #             (core_attention): CoreAttention(
    #               (attention_dropout): Dropout(p=0.0, inplace=False)
    #             )
    #             (dense): Linear(in_features=4096, out_features=4096, bias=False)
    #           )
    #           (post_attention_layernorm): RMSNorm()
    #           (mlp): MLP(
    #             (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)
    #             (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)
    #           )
    #         )
    #       )
    #       (final_layernorm): RMSNorm()
    #     )
    #     (output_layer): Linear(in_features=4096, out_features=65024, bias=False)
    #   )
    # )
    print(model)

    # =========================================================================================================
    # Layer (type:depth-idx)                                  Output Shape              Param #
    # =========================================================================================================
    # ChatGLMForConditionalGeneration                         [512, 16, 2, 128]         --
    # â”œâ”€ChatGLMModel: 1-1                                     [512, 16, 2, 128]         --
    # â”‚    â””â”€Embedding: 2-1                                   [512, 16, 4096]           --
    # â”‚    â”‚    â””â”€Embedding: 3-1                              [16, 512, 4096]           266,338,304
    # â”‚    â””â”€RotaryEmbedding: 2-2                             [8192, 32, 2]             --
    # â”‚    â””â”€GLMTransformer: 2-3                              [512, 16, 4096]           --
    # â”‚    â”‚    â””â”€ModuleList: 3-2                             --                        5,710,903,296
    # â”‚    â”‚    â””â”€RMSNorm: 3-3                                [512, 16, 4096]           4,096
    # â”‚    â””â”€Linear: 2-4                                      [512, 16, 65024]          266,338,304
    # =========================================================================================================
    # Total params: 6,243,584,000
    # Trainable params: 6,243,584,000
    # Non-trainable params: 0
    # Total mult-adds (Units.TERABYTES): 3.06
    # =========================================================================================================
    # Input size (MB): 0.03
    # Forward/backward pass size (MB): 46791.66
    # Params size (MB): 12487.17
    # Estimated Total Size (MB): 59278.86
    # =========================================================================================================
    summary(model, input_size=(16, 512), dtypes=[torch.int])
    model = model.eval()

    # ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM3-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
    response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
    print(response)
    # print_history_message_list(history)

    # 1. å°è¯•æ”¾æ¾èº«å¿ƒï¼Œå¦‚æ·±å‘¼å¸ã€å†¥æƒ³æˆ–æ¸©å’Œçš„ç‘œä¼½ã€‚
    # 2. é¿å…åˆºæ¿€æ€§é£Ÿç‰©å’Œé¥®æ–™ï¼Œå¦‚å’–å•¡ã€èŒ¶å’Œå·§å…‹åŠ›ã€‚
    # 3. ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨ã€‚
    # 4. å°è¯•èˆ’é€‚çš„ç¯å¢ƒï¼Œå¦‚è°ƒæš—ç¯å…‰ã€ä½¿ç”¨ç™½å™ªéŸ³æˆ–èˆ’é€‚çš„åºŠå«ã€‚
    # 5. é¿å…åœ¨æ™šä¸Šè¿‡åº¦ä½¿ç”¨ç”µå­è®¾å¤‡ï¼Œå¦‚æ‰‹æœºã€å¹³æ¿ç”µè„‘å’Œç”µè§†ã€‚
    # 6. ä¿æŒé€‚åº¦çš„è¿åŠ¨ï¼Œå¦‚æ•£æ­¥ã€ç‘œä¼½æˆ–ä¼¸å±•è¿åŠ¨ã€‚
    # 7. å¦‚æœéœ€è¦ï¼Œå¯ä»¥è€ƒè™‘é‡‡ç”¨æ”¾æ¾æŠ€å·§ï¼Œå¦‚æ¸è¿›æ€§è‚Œè‚‰æ¾å¼›æˆ–å‘¼å¸ç»ƒä¹ ã€‚
    # 8. ç¡å‰é€‚å½“é™åˆ¶ä½¿ç”¨å…´å¥‹å‰‚ï¼Œå¦‚å°¼å¤ä¸å’Œé…’ç²¾ã€‚
    # 9. ç¡å‰å°è¯•å†¥æƒ³æˆ–æ·±åº¦æ”¾æ¾ç»ƒä¹ ã€‚
    # 10. å¦‚æœ‰å¿…è¦ï¼Œå¯å’¨è¯¢åŒ»ç”Ÿæˆ–ä¸“ä¸šå¿ƒç†å¥åº·ä¸“å®¶ã€‚
    response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠï¼Œå›å¤å­—æ•°ä¸è¦è¶…è¿‡ 100 ä¸ª", history=history)
    print(response)
    # print_history_message_list(history)

    # 1. å°è¯•è°ƒæ•´å’–å•¡å› æ‘„å…¥é‡ï¼Œæ§åˆ¶åœ¨ä¸€æ—¥ limit å†…ã€‚
    # 2. å°è¯•å…¶ä»–éå’–å•¡å› çš„æç¥é¥®æ–™ï¼Œå¦‚èŒ¶ã€æœæ±æˆ–è‹æ‰“æ°´ã€‚
    # 3. è€ƒè™‘é‡‡ç”¨æ”¾æ¾æŠ€å·§ï¼Œå¦‚å†¥æƒ³æˆ–æ·±åº¦æ”¾æ¾ç»ƒä¹ ã€‚
    # 4. å¢åŠ ç™½å¤©ä¼‘æ¯æ—¶é—´ï¼Œå¦‚å°æ†©æˆ–åˆç¡ã€‚
    # 5. è°ƒæ•´é¥®é£Ÿç»“æ„ï¼Œå¢åŠ æ˜“æ¶ˆåŒ–çš„é£Ÿç‰©ï¼Œå¦‚åšæœã€å…¨éº¦é¢åŒ…æˆ–é¦™è•‰ã€‚
    # 6. å°è¯•è¿›è¡Œæœ‰æ°§è¿åŠ¨ï¼Œå¦‚è·‘æ­¥æˆ–æ¸¸æ³³ã€‚
    # 7. ä¿æŒè‰¯å¥½çš„ç¡çœ æ—¶é—´è¡¨ï¼Œå°½é‡åœ¨åŒä¸€æ—¶é—´å…¥ç¡å’Œèµ·åºŠã€‚
    # 8. é¿å…åœ¨ç¡å‰è¿‡åº¦ä½¿ç”¨ç”µå­è®¾å¤‡ï¼Œå¦‚æ‰‹æœºã€å¹³æ¿ç”µè„‘å’Œç”µè§†ã€‚
    # 9. ç¡å‰é€‚å½“é™åˆ¶å’–å•¡å› æ‘„å…¥ï¼Œå¦‚å‡å°‘å’–å•¡æˆ–èŒ¶æ‘„å…¥é‡ã€‚
    # 10. å¦‚æœ‰å¿…è¦ï¼Œå¯å’¨è¯¢åŒ»ç”Ÿæˆ–ä¸“ä¸šå¿ƒç†å¥åº·ä¸“å®¶ã€‚
    response, history = model.chat(tokenizer, "ä½†æˆ‘å·¥ä½œçš„åŸå› å¿…é¡»å–å’–å•¡ï¼Œå›å¤å­—æ•°ä¸è¦è¶…è¿‡ 100 ä¸ª", history=history)
    print(response)
    # print_history_message_list(history)

    # æˆ‘æ˜ç™½æ‚¨çš„å·¥ä½œåŸå› éœ€è¦å–å’–å•¡æ¥ä¿æŒæ¸…é†’å’Œæé«˜å·¥ä½œæ•ˆç‡ã€‚å’–å•¡å› æ˜¯ä¸€ç§å…´å¥‹å‰‚ï¼Œå¯ä»¥å¢åŠ è­¦è§‰æ€§å’Œæ³¨æ„åŠ›ï¼Œå¸®åŠ©æ‚¨æ›´å¥½åœ°åº”å¯¹æ—¥å¸¸ä»»åŠ¡ã€‚å½“ç„¶ï¼Œé€‚é‡é¥®ç”¨å’–å•¡å¯¹å¤§å¤šæ•°äººæ¥è¯´æ˜¯å®‰å…¨çš„ï¼Œä½†è¯·æ³¨æ„ä¸è¦è¿‡é‡æ‘„å…¥å’–å•¡å› ï¼Œä»¥å…å‡ºç°ä¸è‰¯ååº”ã€‚
    # å†å²å¯¹è¯éœ€è¦é€šè¿‡ä¼ å…¥ history æ¥å¼•å…¥ï¼Œå¦åˆ™æ¨¡å‹è®°ä¸ä½ä¸Šä¸‹æ–‡
    response, history = model.chat(tokenizer, "ä½†æˆ‘å·¥ä½œçš„åŸå› å¿…é¡»å–å’–å•¡ï¼Œå›å¤å­—æ•°ä¸è¦è¶…è¿‡ 100 ä¸ª", history=[])
    print(response)
    # print_history_message_list(history)


def main():
    # 2.2.0+cu121
    print(torch.__version__)
    check_gpu(True)
    check_mul()
    check_mean_op()
    check_std_op()
    other_simple()
    check_batch_norm()
    check_layer_norm()
    check_instance_norm()
    check_group_norm()
    check_weight_norm()
    check_half()
    check_chatglm3()


if __name__ == '__main__':
    main()
